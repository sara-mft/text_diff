# -*- coding: utf-8 -*-
"""
=======================================================================
                        MT Quality Evaluation Script
=======================================================================
This script provides a comprehensive framework for evaluating Machine
Translation (MT) outputs using multiple standard metrics.

It is designed for modularity and ease of use, suitable for researchers
and developers working on MT systems.

Metrics Included:
- BLEU (Bilingual Evaluation Understudy)
- NIST (National Institute of Standards and Technology)
- WER (Word Error Rate)
- METEOR (Metric for Evaluation of Translation with Explicit ORdering)
- LEPOR (Length-Penalty, Precision, n-gram Position difference Penalty, and Recall)

=======================================================================
"""

import sys
from typing import List, Dict, Union
import nltk
from nltk.translate.bleu_score import corpus_bleu
from nltk.translate.nist_score import corpus_nist
from nltk.translate.meteor_score import meteor_score
import jiwer
import lepor

# --- Pre-computation Setup ---
# Some metrics like METEOR require NLTK data. This ensures it's available.
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/wordnet')
except nltk.downloader.DownloadError:
    print("Downloading necessary NLTK data (punkt, wordnet)...")
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    print("Download complete.")


class TranslationEvaluator:
    """
    A comprehensive class to evaluate machine translation results.

    This class encapsulates multiple evaluation metrics to provide a holistic
    view of the translation quality.

    Attributes:
        references (List[List[str]]): A list where each item is a list of
                                      one or more reference translations.
        hypotheses (List[str]): A list of the translations generated by the MT system.
    """

    def __init__(self, references: List[List[str]], hypotheses: List[str]):
        """
        Initializes the evaluator with reference and hypothesis sentences.

        Args:
            references (List[List[str]]): A list of reference sentence lists.
                Example: [['ref1_a', 'ref1_b'], ['ref2_a']]
            hypotheses (List[str]): A list of hypothesis sentences.
                Example: ['hyp1', 'hyp2']

        Raises:
            ValueError: If the number of hypotheses does not match the number of
                        reference sets.
        """
        if len(references) != len(hypotheses):
            raise ValueError(
                "The number of hypothesis sentences must match the number of "
                "reference sets."
            )
        self.references = references
        self.hypotheses = hypotheses
        
        # Pre-tokenize for NLTK-based metrics
        self.tokenized_refs = [[ref.split() for ref in ref_group] for ref_group in references]
        self.tokenized_hyps = [hyp.split() for hyp in hypotheses]

    def calculate_bleu(self) -> Dict[str, float]:
        """
        Calculates corpus-level BLEU scores (BLEU-1 to BLEU-4).
        BLEU measures n-gram precision. Higher is better.
        

        Returns:
            A dictionary containing BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores.
        """
        bleu_scores = {
            'BLEU-1': corpus_bleu(self.tokenized_refs, self.tokenized_hyps, weights=(1, 0, 0, 0)),
            'BLEU-2': corpus_bleu(self.tokenized_refs, self.tokenized_hyps, weights=(0, 1, 0, 0)),
            'BLEU-3': corpus_bleu(self.tokenized_refs, self.tokenized_hyps, weights=(0, 0, 1, 0)),
            'BLEU-4': corpus_bleu(self.tokenized_refs, self.tokenized_hyps, weights=(0, 0, 0, 1)),
        }
        return bleu_scores

    def calculate_nist(self) -> float:
        """
        Calculates corpus-level NIST score.
        NIST is similar to BLEU but weights less frequent n-grams more heavily.
        Higher is better.

        Returns:
            The corpus-level NIST score.
        """
        # Note: NLTK's corpus_nist is designed to work on the entire corpus.
        return corpus_nist(self.tokenized_refs, self.tokenized_hyps)

    def calculate_wer(self) -> float:
        """
        Calculates the Word Error Rate (WER).
        WER measures the number of edits (substitutions, deletions, insertions)
        needed to transform the hypothesis into the reference. Lower is better.

        Returns:
            The overall WER for the corpus.
        """
        # jiwer calculates the overall WER for the entire corpus given lists of strings.
        # It handles cases with multiple references by calculating WER against the
        # reference with the minimum Levenshtein distance.
        return jiwer.wer(self.references, self.hypotheses)

    def calculate_meteor(self) -> float:
        """
        Calculates the average METEOR score.
        METEOR considers precision, recall, stemming, and synonymy.
        It is generally better correlated with human judgment than BLEU.
        Higher is better.

        Returns:
            The average METEOR score over the corpus.
        """
        # meteor_score works on single sentences, so we average the scores.
        scores = [
            meteor_score(refs, hyp)
            for refs, hyp in zip(self.tokenized_refs, self.tokenized_hyps)
        ]
        return sum(scores) / len(scores)

    def calculate_lepor(self) -> float:
        """
        Calculates the average LEPOR score.
        LEPOR is a metric that incorporates length penalty, precision, recall,
        and word order. Higher is better.

        Returns:
            The average LEPOR score over the corpus.
        """
        # lepor.lepor_score works on a single hypothesis and its references. We average.
        scores = [
            lepor.lepor_score(hyp, refs)
            for hyp, refs in zip(self.hypotheses, self.references)
        ]
        return sum(scores) / len(scores)

    def evaluate_all(self) -> Dict[str, Union[float, Dict]]:
        """
        Runs all implemented evaluation metrics and returns a consolidated report.

        Returns:
            A dictionary containing the scores for all metrics.
        """
        print("ðŸ”¬ Starting evaluation...")
        scores = {
            "BLEU": self.calculate_bleu(),
            "NIST": self.calculate_nist(),
            "WER": self.calculate_wer(),
            "METEOR": self.calculate_meteor(),
            "LEPOR": self.calculate_lepor(),
        }
        print("âœ… Evaluation complete.")
        return scores

def print_results(scores: Dict[str, Union[float, Dict]]):
    """Prints the evaluation scores in a formatted way."""
    print("\n" + "="*50)
    print("         Machine Translation Evaluation Results")
    print("="*50)
    
    # BLEU Scores
    bleu_scores = scores['BLEU']
    print("\n--- BLEU (Higher is better) ---")
    for key, value in bleu_scores.items():
        print(f"{key:<8}: {value:.4f}")

    # Other Metrics
    print("\n--- Other Metrics ---")
    print(f"NIST   (Higher is better): {scores['NIST']:.4f}")
    print(f"METEOR (Higher is better): {scores['METEOR']:.4f}")
    print(f"LEPOR  (Higher is better): {scores['LEPOR']:.4f}")
    print(f"WER    (Lower is better) : {scores['WER']:.4f}")
    
    print("\n" + "="*50)


# --- Example Usage ---
if __name__ == "__main__":
    # 1. DEFINE YOUR DATA
    # This is a sample dataset. In a real scenario, you would load this
    # from files (e.g., .txt files with one sentence per line).

    # Source sentences (optional, for context)
    source_sentences = [
        "Le chat est assis sur le tapis.",
        "Le renard brun et rapide saute par-dessus le chien paresseux.",
        "Ceci est un test de traduction automatique.",
    ]

    # Reference translations (gold standard)
    # Note the structure: a list of lists. Each inner list can contain
    # one or more correct translations for the corresponding source sentence.
    reference_translations = [
        ["The cat is sitting on the mat."],
        ["The quick brown fox jumps over the lazy dog."],
        ["This is a machine translation test.", "This is a test of machine translation."],
    ]

    # System translations (output from your MT model)
    system_translations = [
        "The cat sat on the mat.",
        "The quick brown fox jumped over the lazy dog.",
        "This is a machine translation test.",
    ]

    # 2. RUN THE EVALUATION
    try:
        # Initialize the evaluator
        evaluator = TranslationEvaluator(reference_translations, system_translations)
        
        # Calculate all scores
        all_scores = evaluator.evaluate_all()
        
        # Print the results in a user-friendly format
        print_results(all_scores)

    except (ValueError, FileNotFoundError) as e:
        print(f"An error occurred: {e}", file=sys.stderr)
        sys.exit(1)
